---
title: 'STAT 306: Group Project'
author: "Group "
output: pdf_document
urlcolor: blue
---

## Group members

#!!!

```{r echo=FALSE, include=FALSE}

#install.packages("ggplot2")
#install.packages("faraway")
#install.packages("data.table")
#install.packages("rgl")
#install.packages("tourr")
#install.packages("ggcorrplot")
#install.packages("Hmisc")
```

```{r echo=FALSE, include=FALSE}
#Call on libraries
library(ggplot2)
library(faraway)
library(data.table)
library(rgl)
library(tourr)
library(ggcorrplot)
library(scatterplot3d)
library(Hmisc)
```

## Introduction
$\\$
This project is going to investigate the relationship between the price of mobile phones and the various features of mobile phones. The goal of this project is to develop a model that can provide a price reference based on various features of mobile phones. The data used in this project is from Kaggle for Mobile Price Classification | Kaggle. This dataset includes several variables, and details about these features are provided below. However, not all of them would be used in the model to simplify model computation and avoid the overfitting problem. 
$\\$


## Data processing
$\\$
There are four levels in the response variable price_range which includes 0, 1, 2 and 3, lower value means lower price. We convert this variable into a categorical variable to distinguish low price and high price. 0 and 1 are changed to price_range = 0, which means low price. 2 and 3 are changed to price_range = 1, which means high price.

```{r echo=FALSE, include=FALSE}
#Load data
ds <- read.csv("Mobile_train.csv",header = T)
head(ds)
```

```{r echo=FALSE}
#convert data as d
d <- ds
d$price_range[d$price_range < 2] <- 0
d$price_range[d$price_range > 1] <- 1
head(d)
dc <- ds
dc$price_range[dc$price_range < 2] <- 0
dc$price_range[dc$price_range > 1] <- 1
```

ds: original data set, never change
d: convert price range to 0 and 1
dc follow d, keep all cate variable



## Data Modeling
$\\$
In order to obtain a model that best fits the data, we used a principal component analysis (PCA) to select the principal components to be included in our model. There are a large number of explanatory variables, so we would use principal component analysis for model selection. It is sorted by variance from large to small, and the first few components could represent a large proportion of total information. 
```{r}
#Pull out categorical variable
dnc <- d
dnc <- dnc[-2]
head(dnc)
dnc <- dnc[-3]
head(dnc)
dnc <- dnc[-4]
head(dnc)
dnc <- dnc[-16]
head(dnc)
dnc <- dnc[-16]
head(dnc)
dnc <- dnc[-15]

head(dnc)
d <- dnc
```

```{r echo=FALSE}
head(d)
comp_d <- princomp(d)
#comp_d
summary(comp_d)

```


```{r echo=FALSE, include=FALSE}
#view the contribution of each explanatory variables
# The eigenvectors as columns in order from large to small
# Each column is an eigenvector and corresponds to the
# loadings used to transform the data
comp_d$loadings

# Coordinates of the individual observations on the principal
# component axes
comp_d$scores

```


```{r echo=FALSE}
#correlation matrix 
cor.d <- round(cor(d), 2)
pmat.d <- cor_pmat(d)
ggcorrplot(cor.d, hc.order=TRUE, type = "lower",title = "Visualization of the Correlation Matrix", lab = F, lab_size = 2, lab_col = "green", insig = "pch", p.mat = pmat.d, ggtheme = ggplot2::theme_update(), show.diag = 0)
# We can see that moderate-to-strong correlations between pc/fc, sc_h/sc_w, pix_h/pix_w, four_g/three_g, ram/price_range
```

```{r echo=FALSE}
# Plot the components to roughly see how well it worked
plot(comp_d)

# A biplot of the first two components and the original axes overlaid
biplot(comp_d, pc.biplot = TRUE, arrow.len=0.1, col=c("royalblue", "orange"), cex=c(0.8, 0.5), expand=1, scale = 1)



```

```{r echo=FALSE}
#Standarized the data
comp_std <- princomp(d, cor = TRUE)
plot(comp_std)

# Proportion of variation represented by each eigenvector
(comp_std$sdev)^2 / sum(comp_std$sdev^2)
```

```{r echo=FALSE}
#Find the cutoff points
screeplot(comp_std, type = "lines")
#looks like 6 is change point
```



```{r echo=FALSE}
comp_reduced <- comp_std$scores[, 1:5]
```




```{r echo=FALSE}
#Fit logistic regression model
glmModel <- glm(d$price_range~comp_reduced,family = binomial(link = "logit"))
glmModel$residuals
summary(glmModel)
```
```{r}
glm.control(maxit = 200)
glmModel <- glm(price_range~.,data=dc, family = binomial(link = "logit"))
summary(glm(price_range~.,data=d, family = binomial(link = "logit")))
summary(glmModel)
```

```{r}
glmModel2 <- glm(price_range~battery_power + int_memory + mobile_wt + n_cores + px_height + px_width + ram,data=d, family = binomial(link = "logit"))
summary(glmModel2)
```
```{r}
d$price_range <- as.factor(d$price_range)
glmModel2 <- glm(price_range~battery_power + int_memory + mobile_wt + n_cores + px_height + px_width + px_width:px_height + ram,data=train, family = binomial(link = "logit"))
summary(glmModel2)ggplot(data=ds,aes(x=price_range)) +
  geom_histogram()
```


##Data Analysis 
$\\$
Through the screeplot(comp_std, type = "lines") of standardized components, it is obvious that the cutoff point is 6 principal components. It shows that the first 6 components represent a large proportion of total information and have a strong relationship with the price of mobile phones. From the comp_d$loadings, we could find the first 6 are -ram, 0.725*px_height+0.689*px_width, battery_power, -0.688*px_height+0.725*px_width, -mobile_wt, and int_memory respectively. Then we would use these 6 components to fit a logit regression model. 
$\\$
##Conclusion
$\\$
The model we fitted that can provide a price reference of mobile phones is based on ram (random access memory in Megabytes), px_height (the resolution of the height of the screen in pixels), px_width (the resolution of the width of the screen in pixels), battery_power (Total energy a battery can store in on time(mAh)), mobile_wt (the weight of mobile phones in grams), and int_memory (internal memory(Gigabytes)). The logit regression model is log(pi/(1-pi)) = 0.7662 + 35.3069*component1 - 0.1457*component2 - 5.2765*component3 + 8.7543*component4 - 0.4014*component5 + 1.4314*component6, where pi is the probability of price_range = 1 given explanatory variables. 


```{r}
library(tidyverse)
library(repr)
set.seed(123)
testint <- sample.int(2000,500)
train <- d[-testint,]
test <- d[testint,]

```

```{r}
comp_t <- princomp(train)
comp_tstd <- princomp(train, cor = TRUE)
screeplot(comp_tstd, type = "lines")
comp_treduced <- comp_tstd$scores[, 1:5]
glmt <- glm(price_range~comp_treduced,family = binomial(link = "logit"), data = train)
prd <- predict.glm(glmModel2, test,type="response")
prd[prd >= 0.5] <- 1;
prd[prd < 0.5] <- 0;
prd
a <- ifelse(test$price_range == prd, 1,0)
sum(a)
```
```{r}
set.seed(123)
testint <- sample.int(2000,500)
train <- dc[-testint,]
test <- dc[testint,]

```

```{r}
summary(glm(price_range~.,data=dc, family = binomial(link = "logit")))
glmModel3 <- glm(price_range~battery_power + int_memory + mobile_wt + n_cores + px_height + px_width + px_width:px_height + ram + wifi,data=train, family = binomial(link = "logit"))
prd <- predict.glm(glmModel2, test,type="response")
prd[prd >= 0.5] <- 1;
prd[prd < 0.5] <- 0;
a <- ifelse(test$price_range == prd, 1,0)
sum(a)/500
```
```{r}
prdp <- predict.glm(glmt, test,type="term")
prdp[prdp >= 0.5] <- 1;
prdp[prdp < 0.5] <- 0;
a1 <- ifelse(test$price_range == prdp, 1,0)
sum(a1)/500
```

